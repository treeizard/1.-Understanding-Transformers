# (Ongoing work) Basic Tutorial for understanding Basic Transformers and Attention Mechanism using code
Primarily Built using the following tutorials:
- https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html
- https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4861s
