{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture Explained\n",
    "\n",
    "## 1. Introduction\n",
    "Transformer architecture initially proposed in the paper [\"Attention is All you Need\"](https://arxiv.org/pdf/1706.03762.pdf) consists of the following major components:\n",
    "1. Tokenizer convert text to token and tokens to mapped to embeddings.\n",
    "2. Positional encoding inject input word-position information\n",
    "3. Self-attention layer contextually encodes the input sequence information. \n",
    "4. Feed Forward layer which operate bit like a static key-value memory. \n",
    "5. Cross-attention decodes output sequence of different inputs and modalities.\n",
    "- Note the key components above is summarised by Vaclav Kosar in his [blog](https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified)\n",
    "\n",
    "While for many people who does not have any Computer Science or ML backgrounds the above terms may sound confusing and very abstract. Please do not worry about this, we will go through the technical details of Transformers and explain each terms one by one. Please Enjoy this notebook.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/fig1.jpg\" alt=\"fishy\" class=\"bg-primary\" width=\"300px\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Background - How did get here?\n",
    "The phrase \"The Turing Test\" is most properly used to refer to a proposal made by the father of Machine Intelligence Alan Turing in 1950. He proposed a certain kind of game which he explained to be \"The Imitation Game\", this test will test if the machine is capable of understand picture and sequence of words just like human can. The test proposes that:\n",
    "1. A machine, a person and an interrogator is to sit in the same room. \n",
    "2. The Interrogator does not which of the other two is the Machine and which is Person.\n",
    "3. The goal of the Machine is to trick the interoogator into thinking it is the Person.\n",
    "- More detailed explanation about the Turing Test can be found [here](https://plato.stanford.edu/entries/turing-test/).\n",
    "\n",
    "Hence to create a machine that is capable of bypassing \"The Turing Test\", researches begin to tackle the core problems of Natural Language Processing and design difference language models that is capable of understanding the symbols and sequence of \"language\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. N-gram Language Models\n",
    "Predicting is difficult - especially about the future, as the old quip goes. But how about predicting something that seems much easier? For example predicting the next few words somone is going to say? For Example:\n",
    "\n",
    "- \"Good afternoon, have you had ....\"\n",
    "- It is possible that the speaker want to ask whether the other person had \"lunch yet\" or \"a meeting with James\" hence we can assign a probability distribution to each of the possible outcome.\n",
    "- The ability to predict the next word would require the model to have some understanding of what the speaker/writer is trying to say. Hence allowing the n-gram model to be used for tasks such as Machine Translation and Grammar Correction.\n",
    "\n",
    "$$\n",
    "P(c_1, c_2 ... c_N) = P(c_1)p(c_2|c_1)p(c_3|c_1c_2)...P(c_N|c_1c_2...c_{N-1})\n",
    "$$\n",
    "$$\n",
    "P(the|\\texttt{its water is so transparent that}) = \\frac{C(\\texttt{its water is so transparent that the})}{C(\\texttt{its water is so transparent that})}\n",
    "$$\n",
    "\n",
    "- With a large enough corpus (text corpus is a dataset of languge sources of many forms), the above probabilistic equation can be estimated. \n",
    "- More on [N-gram model](https://web.stanford.edu/~jurafsky/slp3/3.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Sequence to Sequence Neutal Netorks.\n",
    "Using the previous established language model theory. An architecture known as the sequence-to-sequence (s2s) neural netowks(NNs) is established. s2s NNs are used translate a sequence of symbols from one input sequence to an output sequence. The neural netowk usually used an encoder-decoder architrcture\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "<img src=\"images/fig2.webp\" alt=\"fishy\" class=\"bg-primary\" width=\"300px\" />\n",
    "<figcaption align=\"center\" >\n",
    "    <a href=\"https://culurciello.medium.com/sequence-to-sequence-neural-networks-3d27e72290fe#:~:text=These%20neural%20network%20usually%20use,Yi%20which%20is%20auto%2Dregressive.\">Encoder Decoder Architecture - Eugenio Culurciello</a>\n",
    "  </figcaption>\n",
    "</p>\n",
    "The encoder takes an input sequence x_i and encode a long sequence of symbols into a sequence vector z_i. The decoder takes as input the sequence vector from the encoder and produces the output sequence y_i. The s2s decoder is generally auto-regressive (uses the previous output of the function to predict future outputs).\n",
    "\n",
    "### 1.3.1 Recurrent Neural Networks.\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "<img src=\"images/fig3.webp\" alt=\"fishy\" class=\"bg-primary\" width=\"300px\" />\n",
    "<figcaption align=\"center\" >\n",
    "    <a href=\"https://culurciello.medium.com/sequence-to-sequence-neural-networks-3d27e72290fe#:~:text=These%20neural%20network%20usually%20use,Yi%20which%20is%20auto%2Dregressive.\">RNN Architecture - Eugenio Culurciello</a>\n",
    "  </figcaption>\n",
    "</p>\n",
    "Previously (before GPT taking over) Recurrent Neural Networks (or RNN) is almost the industry standard for natural language processing tasks. RNN and more complex LSTM (which resolved some of the vanishing gradients problem of RNN) built the foundation for many of the old voice assistants such as Siri, Cortana and Alexa. However RNN is not resource friendly what so ever, with the lack of focus on the important part of the sentence (attention) it is difficult to train an RNN model that have the similar level of performance as Transformer model. Sadly, the notebook will not have time to go into RNN, so if you are interested in learning more please head to the following resources:\n",
    "\n",
    "1. [Stanford RNN Cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "\n",
    "2.  <a href=\"https://www.ibm.com/topics/recurrent-neural-networks#:~:text=A%20recurrent%20neural%20network%20(RNN,data%20or%20time%20series%20data.\">RNN IBM</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Self - Attention\n",
    "The concept of \"attention\" is originally designed to improve RNN (but in the end technically killed her) so that RNN will be less expensive in the handling of longer sequences of sentences. However in the famous paper [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf), it is established that with the attention mechanism, a higher quality and more parrallelizable model known as transformers can be created. \n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "<img src=\"images/fig4.png\" alt=\"fishy\" class=\"bg-primary\" width=\"300px\" />\n",
    "<figcaption align=\"center\" >\n",
    "    <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">Attention Mechanism</a>\n",
    "  </figcaption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Transformer Embeddings and Tokenization\n",
    "<b> 1. Input is tokenized, the tokens are then embedded. </b>\n",
    "\n",
    "<b> All the code below are taken from Prof Sebastian Raschka's [blog ](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'cookie': 2, 'eat': 3, 'first': 4, 'is': 5}\n"
     ]
    }
   ],
   "source": [
    "# 1. Define a Sentence\n",
    "sentence = 'Life is cookie, eat cookie first'\n",
    "\n",
    "# 2. Iterate through the Sentence to create a dictionary\n",
    "dc = {s:i for i, s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "# enumerate: iterate over a list, tuple or dictionary and return a tuple containing the index of each element. \n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization converts a text into a list of integers. \n",
    "- With the above code we can see that each of the unique word in the sentence is assigned to a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 5, 2, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 3. We can now conver the sentence from its string form into the tokenalized form. \n",
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2. With this list of tokenised string, we can embed the list of words into a matrix, each of the word can be represented using an n-dimensional vector.<b> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 16])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
      "         -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624],\n",
      "        [ 0.0109, -0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,\n",
      "          0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835],\n",
      "        [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057, -0.7746,\n",
      "         -1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347, -0.4879],\n",
      "        [-0.9138, -0.6581,  0.0780,  0.5258, -0.4880,  1.1914, -0.8140, -0.7360,\n",
      "         -1.4032,  0.0360, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,  1.3835],\n",
      "        [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057, -0.7746,\n",
      "         -1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347, -0.4879],\n",
      "        [ 1.4451,  0.8564,  2.2181,  0.5232,  0.3466, -0.1973, -1.0546,  1.2780,\n",
      "         -0.1722,  0.5238,  0.0566,  0.4263,  0.5750, -0.6417, -2.2064, -0.7508]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "embed = torch.nn.Embedding(6,16)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "# detach(): returns a new tensor, detached from the current graph. <- so the tensor would no longer require gradient descent.\n",
    "# The difference between detach() and no_grad() is that when a tensor is set to detach() it will be detached from the computational graph forever and will not be able to be optimized even after requires_grad flag is set to true.\n",
    "print(embedded_sentence.shape)\n",
    "print(embedded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Token Embedding</b>: Map Tokens to their representations. There are many ways of embedding tokens (representing words into a vector form), the different ways we choose to embed our established tokens extracted from our corpus, will determine the meaning or value we attribute to each words. \n",
    "- One of the most common way to embed text is through the Word2vec method, Word2vec is a two-layer neural net that processes text by \"vectorizing\" words. \n",
    "- Word2vec has been used around since 2013, it has also been shown to be effective in creating recommendation engines and making sense of sequential data in commercial tasks. \n",
    "- While Word2vec is not a deep neural network, it turns text into a numerical form that deep NNs can understand. \n",
    "    - Word2vec group the vectors of similar words together into a vectorspace. That is, it preceives \"similarities\" mathematically, by creating multi-dimensional vectors representations of language features.\n",
    "    - The computed vectors are shown to possess the components of that make up the meaning of the word, one very famous example is the:\n",
    "        <p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig5.png\" alt=\"fishy\" class=\"bg-primary\" width=\"300px\" />\n",
    "        <figcaption align=\"center\" >\n",
    "            <a href=\"https://jalammar.github.io/illustrated-word2vec/\">Word2Vec</a>\n",
    "          </figcaption>\n",
    "        </p>\n",
    "    - To compute the closeness of words to each other for given features. Word2vec uses the cosine similarity (Think of a unit circle, highest cosine similarity occurs at 1, which is measured at 0 degrees) to associate words. \n",
    "    - Word2vec method can be used to train a language model in two ways:\n",
    "        1. <b>CBOW:</b> use context to predict a target word. \n",
    "        2. <b>Skip-gram:</b> use the word to predict target context.\n",
    "For More Details, check out this [blog](https://jalammar.github.io/illustrated-word2vec/).\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3. Defining the Weight Matrices </b>\n",
    "\n",
    "- self-attention mechanism is also known as the scaled dot-product attention. Which is integrated into the transformer architrcture. \n",
    "- self-attention mechanism utilizes three weight matrices, referred to as $W_q, W_k, W_v$, which are adjusted as model parameters during the training process. These three weight matrices operates similar to fc layers, say we have weight of $W$ and embedded inputs of $x$.\n",
    "    $$\n",
    "    q = W_q*x\n",
    "    $$\n",
    "    $$\n",
    "    k = W_k*x\n",
    "    $$\n",
    "    $$\n",
    "    v = W_v*x\n",
    "    $$\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig6.png\" alt=\"fishy\" class=\"bg-primary\" width=\"300px\" />\n",
    "        <figcaption align=\"center\" >\n",
    "            <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">query, key, value</a>\n",
    "          </figcaption>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape: torch.Size([6, 24])\n",
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "# Extract the embeded vector dimension from the embeded sentence matrix\n",
    "d = embedded_sentence.shape[1]\n",
    "# Define the Attention Weight Matrix size\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "# Define the Weight Matrix\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))\n",
    "# Multiply the Weight Matrix with our input\n",
    "query = W_query.matmul(embedded_sentence.T).T\n",
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"query.shape:\", query.shape)\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- People who are familiar with data storage (sql), will definitely be familiar with the three terms \"query\", \"key\" and \"value\". Where Query is the word of interest we want to find in the database. When we multiply query and key together and pass them into the softmax function, we will find the key which best describe the query. If we are to then multiply the result described above with the value, it will correspondingly represent the value we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Computing the Attention Scores</b>\n",
    "$$\n",
    "Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "- The general process of attention mechanism has been described above, the idea of __self-attention__ is to find a layered attention for itself to implement more depth in the computational process. Additionally the extra term $\\frac{1}{\\sqrt{d_k}}$ is used to scale the unnormalized attention weights ($QK^T$). By scaling by $d_k$ ensures that the Euclidean length of the weight vector will be approximately in the same magnitude.\n",
    "\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig7.png\" alt=\"fishy\" class=\"bg-primary\" width=\"500px\" />\n",
    "        <figcaption align=\"center\" >\n",
    "            <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">self attention pipeline</a>\n",
    "          </figcaption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([-2.5894, -1.3496, -1.4880, -2.4979, -0.6985, -2.7224, -3.4506, -1.1927,\n",
      "        -1.8831, -1.4491, -3.3964, -3.1386, -2.3252, -1.2042,  0.2298, -0.4006,\n",
      "        -3.1605, -1.5531, -1.3839, -1.5905, -2.8909, -0.7388, -4.8752, -2.2597,\n",
      "        -1.0470, -1.8028, -3.6478, -0.9484], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# 1. Extract a single word from the embdeded sentence\n",
    "x_2 = embedded_sentence[1]\n",
    "\n",
    "# 2. Compute the query, key and value\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "# 3. Compute the normalized attention weights\n",
    "omega_2 = query_2.matmul(keys.T)\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "\n",
    "# 4. Compute the context vector (how each word relate to the other words)\n",
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Multi-Head Attention\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig8.png\" alt=\"fishy\" class=\"bg-primary\" width=\"500px\" />\n",
    "        <img src=\"images/fig9.png\" alt=\"fishy\" class=\"bg-primary\" width=\"500px\" />\n",
    "        <figcaption align=\"centre\" >\n",
    "            <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">single attention(left) vs multi-head attention(right)</a>\n",
    "          </figcaption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 24, 6])\n",
      "multihead_values.shape: torch.Size([3, 28, 6])\n"
     ]
    }
   ],
   "source": [
    "# Assume we have 3 attention heads\n",
    "h = 3\n",
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))\n",
    "\n",
    "# Do the matrix multiplication similar to single attention\n",
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)\n",
    "\n",
    "# Repeat the weight matrix \n",
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cross Attention\n",
    "\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig10.png\" alt=\"fishy\" class=\"bg-primary\" width=\"500px\" />\n",
    "        <figcaption align=\"centre\" >\n",
    "            <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">cross-attention summary</a>\n",
    "          </figcaption>\n",
    "</p>\n",
    "\n",
    "- The multi-head attention is somewhat un-interesting (it is very similar to the channel concept of CNN), what is truly interesting is the cross-attention. In cross-attention, we mix or combine two different input sequences. In the case of the original transformer architecture below, that's the sequence returned by the encoder module on the left and the input sequence being processed by the decorder part on the right. Cross-attention is useful when we go from an input sentence to an output sentence in the context of language translation. (stable diffusion also used corss attention extensively).\n",
    "\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig11.png\" alt=\"fishy\" class=\"bg-primary\" width=\"500px\" />\n",
    "        <figcaption align=\"centre\" >\n",
    "            <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">Original Transformer</a>\n",
    "          </figcaption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. The Mathematical Trick in self-attention\n",
    "- Tokens should only be able to look back the token at the 6th location should only be able to talk to tokens say at 5th, 3rd location. They should not be able to see the tokens at the 8th or 9th location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2\n",
    "X = torch.randn(B,T,C)\n",
    "print(X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "# For each batch <- which will be independent from one another\n",
    "for b in range(B):\n",
    "    # We go through each batch and for each T\n",
    "    for t in range(T):\n",
    "        xprev = X[b,:t+1]\n",
    "        # We compute the average\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Embedding\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "New Embedding\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Embedding\")\n",
    "print(X[0])\n",
    "print(\"New Embedding\")\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the original embedding to the average embedding we have create. We can see that the two tensors both have the same first row and for each of the proceding elements as we pass through time, we can see that the future elements are the average of all the previous elements. Computing attention this way is very similar to a horrible RNN, a way to improve this is through the usage of a \"mathematical trick\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig12.png\" alt=\"fishy\" class=\"bg-primary\" width=\"500px\" />\n",
    "        <figcaption align=\"centre\" >\n",
    "            <a href=\"https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra/blob/main/The-Art-of-Linear-Algebra.pdf\">The Art of Linear Algebra</a>\n",
    "          </figcaption>\n",
    "</p>\n",
    "\n",
    "- Using this trick we no longer need large amount of excessive for loops like what we have showed above, we just need matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6667, 2.3333],\n",
      "        [2.6667, 3.6667],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Now using the lower triangle function in the torch and normalising the rows, we can:\n",
    "a2 = torch.tril(torch.ones(3,3))/torch.sum(a, 1)\n",
    "c2 = a2 @ b\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Transformer\n",
    "\n",
    "__Now__, we will follow the Youtube Tutorial Provided by Mr Andrej Karpathy and create/train a tiny GPT model from scretch, if you find my words and instructions are unclear (which is quite likly tbh), please visit the [original tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5869s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Our Data\n",
    "- We have extracted our data from the github repository [tiny shakespare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset in characters: 1115393\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length of the dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Extract all the unique characters from the data\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike before, where we tokenized the text based on words, here we would like to tokenize based on characters (alphebet).\n",
    "- In practice, you probably don't need to write the encoding function from scretch, you could just use packages such as GPT2's [tiktoken](https://github.com/openai/tiktoken). \n",
    "- __Aside:__ A lambda function is a small anonymous function, it can take any number of arguments, but can only have one expression.\n",
    "    - x = lambda a:a+10 <- the left side of the \":\" is the input and the right side is the output.\n",
    "    - lambda s: [stoi[c] for c in s] <- this will take in s (which is a string) and iterate through each of the characters and convert them into encoded numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# This component of code will create a mapping of characters to number in the form of a dictionary (just like what we did)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } #character -> number\n",
    "itos = { i:ch for i,ch in enumerate(chars) } #number -> character\n",
    "\n",
    "# create an encoder function that encodes words based on characters. \n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Encode everything in the training dataset.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train val data split.\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we have mentioned previously, the model is a sequential model, therefore it is essential for us to define the block size of the sequential data which we will be use to train the model.\n",
    "- The model will predict the next words based on previously known information. e.g. predict 47 based on 18, predict 56 based on 47 and 18 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]\n",
    "\n",
    "# Some code to illustrate the training process.\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just like normal neural-network to imporve the processing capabilities (python can use multi-processing owo) and model accuracy, we will also be implementing mini batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
      "----\n",
      "when input is [53] the target: 59\n",
      "when input is [53, 59] the target: 6\n",
      "when input is [53, 59, 6] the target: 1\n",
      "when input is [53, 59, 6, 1] the target: 58\n",
      "when input is [53, 59, 6, 1, 58] the target: 56\n",
      "when input is [53, 59, 6, 1, 58, 56] the target: 47\n",
      "when input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
      "when input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n",
      "when input is [49] the target: 43\n",
      "when input is [49, 43] the target: 43\n",
      "when input is [49, 43, 43] the target: 54\n",
      "when input is [49, 43, 43, 54] the target: 1\n",
      "when input is [49, 43, 43, 54, 1] the target: 47\n",
      "when input is [49, 43, 43, 54, 1, 47] the target: 58\n",
      "when input is [49, 43, 43, 54, 1, 47, 58] the target: 1\n",
      "when input is [49, 43, 43, 54, 1, 47, 58, 1] the target: 58\n",
      "when input is [13] the target: 52\n",
      "when input is [13, 52] the target: 45\n",
      "when input is [13, 52, 45] the target: 43\n",
      "when input is [13, 52, 45, 43] the target: 50\n",
      "when input is [13, 52, 45, 43, 50] the target: 53\n",
      "when input is [13, 52, 45, 43, 50, 53] the target: 8\n",
      "when input is [13, 52, 45, 43, 50, 53, 8] the target: 0\n",
      "when input is [13, 52, 45, 43, 50, 53, 8, 0] the target: 26\n",
      "when input is [1] the target: 39\n",
      "when input is [1, 39] the target: 1\n",
      "when input is [1, 39, 1] the target: 46\n",
      "when input is [1, 39, 1, 46] the target: 53\n",
      "when input is [1, 39, 1, 46, 53] the target: 59\n",
      "when input is [1, 39, 1, 46, 53, 59] the target: 57\n",
      "when input is [1, 39, 1, 46, 53, 59, 57] the target: 43\n",
      "when input is [1, 39, 1, 46, 53, 59, 57, 43] the target: 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "n_embed = 32\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Random offset into the training set.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, it has been established that we have a batched dataset, we will begin feeding this batached dataset into the neural network.\n",
    "- We will explain a bit more about the idea of word embedding here: \n",
    "    - nn.Embedding(vocab_size, embedding_dim) <- the vocab size represent the size of the vocabulary in this case how much unique characters are we going to capture in a model. \n",
    "        - if we have say nn.Embedding(6, 7) and a 24 words as input. Then the output matrix will be [24, 7] tensor, the model however will only be allow for 6 unique words. \n",
    "### 3.2. Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor([[-0.0372,  0.5101, -0.3296,  ...,  0.4318, -0.6124,  0.2609],\n",
      "        [ 0.5865, -0.1595, -0.0942,  ...,  0.3435,  1.2050, -0.8776],\n",
      "        [-0.0270,  0.9009,  0.6174,  ...,  0.5172,  0.5898, -0.1269],\n",
      "        ...,\n",
      "        [ 0.3226, -0.4082, -0.2581,  ...,  0.1849,  1.4394,  0.7642],\n",
      "        [ 0.4565, -0.8794,  0.3228,  ..., -0.2943, -0.0605,  1.5607],\n",
      "        [-0.1998, -0.1509,  0.0761,  ..., -0.7685,  1.0741,  0.2044]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.3152, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "from torch.nn import functional as f \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# This is the sample code for the cration of a bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed):\n",
    "        super().__init__()\n",
    "        # We have demonstrated the usage of nn.Embedding in the previous sections. \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # We create a custom embedding layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "       \n",
    "        \n",
    "            # - we have previously defined vocab_size to be the number of unique characters from out input.txt.\n",
    "            # - embedding showns how each character relate to one and another. \n",
    "    \n",
    "    # The Forward pass function is the essential component of the neural network\n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        pos_idx = []\n",
    "\n",
    "        # The code provided does not written correctly, I made some modification here (it is very inefficient)\n",
    "        for i in range(T):\n",
    "            if (i<8):\n",
    "                pos_idx.append(i)\n",
    "            else:\n",
    "                pos_idx.append(i%8)\n",
    "        pos_idx=torch.tensor(pos_idx)\n",
    "        # idx and targets are both (B,T) tensor of integer\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C1)\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # We add some positional embedding. <- for bigram model the positional embedding is not particularly useful\n",
    "        \n",
    "        #print(T)\n",
    "        #print(token_emb.shape)\n",
    "        #print(pos_emb.shape)\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "         # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # We calculate the cross entroppy loss \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    # Generate function can be used to validate our model\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "           \n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            #print(logits.shape)\n",
    "            \n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            #print(idx.shape)\n",
    "        return idx\n",
    "    \n",
    "# We have defined the vocab size in previous section\n",
    "m = BigramLanguageModel(vocab_size=vocab_size, n_embed=n_embed)\n",
    "# xb and yb as shown in the above code chunk is the input and the target.\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "&Ks.O&HmjJQOOSiekNGqWFMPy'haCchX.w.qBYMD\n",
      "E.lID!Ddn\n",
      "?$xC?x?wH;,xG&k&tWuyUMI.JhRsXUyg-NwSimQaltzavenDI\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.429715633392334\n",
      "4.524538993835449\n",
      "4.400267124176025\n",
      "4.470452785491943\n",
      "4.367921829223633\n",
      "4.387684345245361\n",
      "4.3218488693237305\n",
      "4.357346534729004\n",
      "4.3777337074279785\n",
      "4.338712692260742\n",
      "4.239202976226807\n",
      "4.310429096221924\n",
      "4.291584014892578\n",
      "4.215610980987549\n",
      "4.142261981964111\n",
      "4.225683212280273\n",
      "4.204963684082031\n",
      "4.227346420288086\n",
      "4.187575817108154\n",
      "4.192535877227783\n",
      "4.215929985046387\n",
      "4.169702529907227\n",
      "4.087116718292236\n",
      "4.243812084197998\n",
      "4.0292253494262695\n",
      "4.091665744781494\n",
      "4.085590362548828\n",
      "4.082399845123291\n",
      "4.0830817222595215\n",
      "4.054449081420898\n",
      "4.008861541748047\n",
      "4.01439905166626\n",
      "3.9793031215667725\n",
      "3.9738619327545166\n",
      "3.9716131687164307\n",
      "3.8572936058044434\n",
      "3.9882524013519287\n",
      "3.962925910949707\n",
      "3.9457595348358154\n",
      "3.8942606449127197\n",
      "3.978365182876587\n",
      "3.910951852798462\n",
      "4.031850814819336\n",
      "3.8615918159484863\n",
      "3.80857253074646\n",
      "3.8087329864501953\n",
      "3.867537260055542\n",
      "3.827179193496704\n",
      "3.8883941173553467\n",
      "3.858880043029785\n",
      "3.820722818374634\n",
      "3.791415214538574\n",
      "3.759516954421997\n",
      "3.7909579277038574\n",
      "3.6848134994506836\n",
      "3.7550389766693115\n",
      "3.7117972373962402\n",
      "3.743173360824585\n",
      "3.745675802230835\n",
      "3.782256841659546\n",
      "3.6874492168426514\n",
      "3.7617218494415283\n",
      "3.6266849040985107\n",
      "3.573108196258545\n",
      "3.6815671920776367\n",
      "3.7161343097686768\n",
      "3.6219942569732666\n",
      "3.7097790241241455\n",
      "3.605644702911377\n",
      "3.762629985809326\n",
      "3.694530725479126\n",
      "3.7036094665527344\n",
      "3.6084413528442383\n",
      "3.556652307510376\n",
      "3.6425769329071045\n",
      "3.4880967140197754\n",
      "3.4752652645111084\n",
      "3.5122618675231934\n",
      "3.603548049926758\n",
      "3.503871440887451\n",
      "3.4911539554595947\n",
      "3.5380871295928955\n",
      "3.4482338428497314\n",
      "3.4536099433898926\n",
      "3.488772392272949\n",
      "3.6163878440856934\n",
      "3.5435211658477783\n",
      "3.5435070991516113\n",
      "3.5075929164886475\n",
      "3.521899461746216\n",
      "3.5171494483947754\n",
      "3.496227741241455\n",
      "3.4679763317108154\n",
      "3.4557175636291504\n",
      "3.542719602584839\n",
      "3.5457160472869873\n",
      "3.376525402069092\n",
      "3.3781256675720215\n",
      "3.441335439682007\n",
      "3.3620355129241943\n",
      "3.3162999153137207\n",
      "3.4806907176971436\n",
      "3.3835644721984863\n",
      "3.3033246994018555\n",
      "3.3666880130767822\n",
      "3.3145954608917236\n",
      "3.354447841644287\n",
      "3.3773648738861084\n",
      "3.424546241760254\n",
      "3.346266508102417\n",
      "3.351651430130005\n",
      "3.2318217754364014\n",
      "3.2741639614105225\n",
      "3.285655975341797\n",
      "3.2856874465942383\n",
      "3.2847630977630615\n",
      "3.245577812194824\n",
      "3.3661961555480957\n",
      "3.148322820663452\n",
      "3.2574617862701416\n",
      "3.2895236015319824\n",
      "3.4179768562316895\n",
      "3.3269782066345215\n",
      "3.16156268119812\n",
      "3.115427017211914\n",
      "3.100823163986206\n",
      "3.3592541217803955\n",
      "3.1583313941955566\n",
      "3.1590166091918945\n",
      "3.2618911266326904\n",
      "3.1241910457611084\n",
      "3.264986515045166\n",
      "3.2161831855773926\n",
      "3.2201645374298096\n",
      "3.416360855102539\n",
      "3.3036961555480957\n",
      "3.167614221572876\n",
      "3.02938175201416\n",
      "3.261207342147827\n",
      "3.2830288410186768\n",
      "3.072277784347534\n",
      "3.251093626022339\n",
      "3.209928274154663\n",
      "3.280553102493286\n",
      "3.2328414916992188\n",
      "3.1841001510620117\n",
      "3.1219472885131836\n",
      "3.197540760040283\n",
      "3.2859129905700684\n",
      "2.9980742931365967\n",
      "3.212394952774048\n",
      "3.0490915775299072\n",
      "3.2202160358428955\n",
      "3.046050548553467\n",
      "3.0841946601867676\n",
      "3.155601739883423\n",
      "3.2512435913085938\n",
      "3.1708242893218994\n",
      "2.888274908065796\n",
      "3.1901049613952637\n",
      "3.0522449016571045\n",
      "3.033881187438965\n",
      "3.1266582012176514\n",
      "3.0678553581237793\n",
      "3.143343687057495\n",
      "3.061837673187256\n",
      "3.041512966156006\n",
      "3.2666943073272705\n",
      "2.9714345932006836\n",
      "3.2447822093963623\n",
      "3.1382360458374023\n",
      "3.151500940322876\n",
      "2.9075613021850586\n",
      "3.1720330715179443\n",
      "3.0282046794891357\n",
      "3.027526378631592\n",
      "3.238009214401245\n",
      "3.1542000770568848\n",
      "3.1076483726501465\n",
      "3.2140920162200928\n",
      "3.0968799591064453\n",
      "3.161647319793701\n",
      "3.120666742324829\n",
      "3.1929101943969727\n",
      "2.9934518337249756\n",
      "3.015292167663574\n",
      "2.9598124027252197\n",
      "2.9566938877105713\n",
      "3.04518461227417\n",
      "3.0184073448181152\n",
      "2.9888250827789307\n",
      "3.005610942840576\n",
      "3.0321128368377686\n",
      "3.0737600326538086\n",
      "2.933145523071289\n",
      "2.9657843112945557\n",
      "2.9020371437072754\n",
      "2.963610887527466\n",
      "2.8168649673461914\n",
      "3.0272881984710693\n",
      "3.1690690517425537\n",
      "3.0661098957061768\n",
      "3.0032389163970947\n",
      "3.1196398735046387\n",
      "3.059375047683716\n",
      "2.936596632003784\n",
      "2.969059944152832\n",
      "2.9600439071655273\n",
      "2.9674019813537598\n",
      "2.9635825157165527\n",
      "3.059072256088257\n",
      "3.052560567855835\n",
      "2.9049041271209717\n",
      "3.136451482772827\n",
      "2.9703283309936523\n",
      "3.133617639541626\n",
      "3.055102825164795\n",
      "2.902111768722534\n",
      "2.763172149658203\n",
      "2.908111333847046\n",
      "3.0288684368133545\n",
      "3.0439980030059814\n",
      "2.9527971744537354\n",
      "3.058055877685547\n",
      "2.8862156867980957\n",
      "2.9598915576934814\n",
      "2.7682301998138428\n",
      "2.950143814086914\n",
      "2.8602945804595947\n",
      "2.923535108566284\n",
      "3.0200557708740234\n",
      "3.0510308742523193\n",
      "2.975198745727539\n",
      "3.0543999671936035\n",
      "3.026677370071411\n",
      "2.8872294425964355\n",
      "2.935299873352051\n",
      "2.8374125957489014\n",
      "2.9699532985687256\n",
      "2.9232351779937744\n",
      "2.892603635787964\n",
      "2.9520962238311768\n",
      "3.103703737258911\n",
      "2.8389172554016113\n",
      "2.9987268447875977\n",
      "3.022378921508789\n",
      "2.901885986328125\n",
      "3.079695463180542\n",
      "2.859832763671875\n",
      "2.8805289268493652\n",
      "2.9825644493103027\n",
      "2.974640130996704\n",
      "3.10184383392334\n",
      "2.8981876373291016\n",
      "2.908090591430664\n",
      "2.8702151775360107\n",
      "2.770129442214966\n",
      "2.855715274810791\n",
      "2.82249116897583\n",
      "2.903953790664673\n",
      "3.130850076675415\n",
      "2.89680814743042\n",
      "2.938378095626831\n",
      "3.1053779125213623\n",
      "2.9644699096679688\n",
      "2.8457541465759277\n",
      "2.894758701324463\n",
      "2.935553789138794\n",
      "2.8845882415771484\n",
      "2.8284244537353516\n",
      "2.873563051223755\n",
      "2.775390625\n",
      "2.9192683696746826\n",
      "2.8505969047546387\n",
      "2.803745985031128\n",
      "2.8784871101379395\n",
      "3.0261263847351074\n",
      "2.757737159729004\n",
      "3.0188117027282715\n",
      "3.052549123764038\n",
      "2.847391128540039\n",
      "2.8158161640167236\n",
      "2.957902669906616\n",
      "2.9073667526245117\n",
      "2.864168167114258\n",
      "3.096780776977539\n",
      "2.8845484256744385\n",
      "2.751384735107422\n",
      "2.877659797668457\n",
      "2.6957240104675293\n",
      "2.891796588897705\n",
      "2.8454904556274414\n",
      "2.833831310272217\n",
      "2.8768436908721924\n",
      "2.911210060119629\n",
      "2.985517978668213\n",
      "2.9273107051849365\n",
      "2.882126808166504\n",
      "2.8697192668914795\n",
      "2.8186168670654297\n",
      "2.791822671890259\n",
      "2.7813868522644043\n",
      "2.9299468994140625\n",
      "2.768587589263916\n",
      "2.8390002250671387\n",
      "2.796783208847046\n",
      "2.842027425765991\n",
      "2.8705883026123047\n",
      "2.687746286392212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.829002618789673\n",
      "2.83974552154541\n",
      "2.8202767372131348\n",
      "2.96455979347229\n",
      "2.9795970916748047\n",
      "2.815258026123047\n",
      "2.7606472969055176\n",
      "2.8205089569091797\n",
      "3.0862538814544678\n",
      "2.7197582721710205\n",
      "2.9499659538269043\n",
      "3.025893211364746\n",
      "2.7619950771331787\n",
      "2.814711332321167\n",
      "2.979884147644043\n",
      "2.9425296783447266\n",
      "2.769573211669922\n",
      "2.824856996536255\n",
      "2.7914462089538574\n",
      "3.0634653568267822\n",
      "2.784821033477783\n",
      "2.815312623977661\n",
      "2.791013240814209\n",
      "2.8447909355163574\n",
      "2.9342293739318848\n",
      "2.822354316711426\n",
      "2.893488883972168\n",
      "2.8470354080200195\n",
      "2.82509708404541\n",
      "2.8932230472564697\n",
      "2.994302988052368\n",
      "2.9407198429107666\n",
      "2.9579432010650635\n",
      "2.90751051902771\n",
      "2.847487211227417\n",
      "2.7600510120391846\n",
      "2.7437832355499268\n",
      "2.835254430770874\n",
      "3.0383858680725098\n",
      "2.708141803741455\n",
      "2.792759895324707\n",
      "2.894479751586914\n",
      "3.1124768257141113\n",
      "2.8530986309051514\n",
      "2.813288927078247\n",
      "2.9875264167785645\n",
      "2.778353452682495\n",
      "2.6375060081481934\n",
      "2.8899102210998535\n",
      "2.644167184829712\n",
      "2.784364938735962\n",
      "2.8192663192749023\n",
      "2.8054146766662598\n",
      "2.798884391784668\n",
      "2.8601438999176025\n",
      "2.7003636360168457\n",
      "2.9015438556671143\n",
      "2.9195961952209473\n",
      "2.7517776489257812\n",
      "2.7166013717651367\n",
      "2.9511559009552\n",
      "2.8791680335998535\n",
      "2.8432414531707764\n",
      "2.8013734817504883\n",
      "2.597238779067993\n",
      "2.828648090362549\n",
      "2.74306583404541\n",
      "2.6982855796813965\n",
      "2.818695545196533\n",
      "2.858548879623413\n",
      "2.7401115894317627\n",
      "2.7210981845855713\n",
      "2.651750326156616\n",
      "2.7148818969726562\n",
      "2.7573516368865967\n",
      "2.8609471321105957\n",
      "2.945497751235962\n",
      "2.7857420444488525\n",
      "2.7628819942474365\n",
      "2.7562742233276367\n",
      "2.6848537921905518\n",
      "2.9332356452941895\n",
      "2.720130205154419\n",
      "2.914961814880371\n",
      "2.8232362270355225\n",
      "2.741260051727295\n",
      "2.746891498565674\n",
      "2.748150110244751\n",
      "2.823038101196289\n",
      "3.0162549018859863\n",
      "2.950258493423462\n",
      "2.7744319438934326\n",
      "2.684069871902466\n",
      "2.857759952545166\n",
      "2.865208864212036\n",
      "2.8912086486816406\n",
      "2.7541921138763428\n",
      "2.7049620151519775\n",
      "2.949978828430176\n",
      "2.7811968326568604\n",
      "2.843076229095459\n",
      "2.7958824634552\n",
      "2.734680652618408\n",
      "2.715655565261841\n",
      "2.9102418422698975\n",
      "2.674269914627075\n",
      "2.6800730228424072\n",
      "2.6911072731018066\n",
      "2.781534194946289\n",
      "2.9005632400512695\n",
      "2.6103909015655518\n",
      "2.766300916671753\n",
      "2.6791810989379883\n",
      "2.883274793624878\n",
      "2.661900520324707\n",
      "2.882448434829712\n",
      "2.697575092315674\n",
      "2.8935365676879883\n",
      "2.866107702255249\n",
      "2.9441030025482178\n",
      "2.7567825317382812\n",
      "2.8231236934661865\n",
      "2.7268807888031006\n",
      "2.725555419921875\n",
      "2.66660737991333\n",
      "2.805588960647583\n",
      "2.719000816345215\n",
      "2.724700927734375\n",
      "2.7836341857910156\n",
      "2.5292246341705322\n",
      "2.7980198860168457\n",
      "2.732306957244873\n",
      "2.9373252391815186\n",
      "2.7477736473083496\n",
      "2.6631462574005127\n",
      "2.878363847732544\n",
      "2.7449533939361572\n",
      "2.794555425643921\n",
      "2.7597274780273438\n",
      "2.617997407913208\n",
      "2.826491594314575\n",
      "2.788745641708374\n",
      "2.812469005584717\n",
      "2.7325613498687744\n",
      "2.6415460109710693\n",
      "2.662202835083008\n",
      "2.6979007720947266\n",
      "2.7402100563049316\n",
      "2.6784799098968506\n",
      "2.7107150554656982\n",
      "2.7070629596710205\n",
      "2.878945827484131\n",
      "2.693342924118042\n",
      "2.766413450241089\n",
      "2.7957162857055664\n",
      "2.792426109313965\n",
      "2.600964069366455\n",
      "2.7933197021484375\n",
      "2.7077879905700684\n",
      "2.7714381217956543\n",
      "2.6181445121765137\n",
      "3.021388053894043\n",
      "2.7741143703460693\n",
      "2.6385257244110107\n",
      "2.6712677478790283\n",
      "2.711092710494995\n",
      "2.7410809993743896\n",
      "2.7326512336730957\n",
      "2.7195184230804443\n",
      "2.7387442588806152\n",
      "2.677363157272339\n",
      "2.722780227661133\n",
      "2.7026829719543457\n",
      "2.864560604095459\n",
      "2.825709819793701\n",
      "2.7056190967559814\n",
      "2.615408420562744\n",
      "2.7189855575561523\n",
      "2.6820123195648193\n",
      "2.635906219482422\n",
      "2.753840446472168\n",
      "2.6996355056762695\n",
      "2.722372531890869\n",
      "2.8073549270629883\n",
      "2.8247694969177246\n",
      "2.927548408508301\n",
      "2.8417606353759766\n",
      "2.6600661277770996\n",
      "2.7446236610412598\n",
      "2.6848995685577393\n",
      "2.7679545879364014\n",
      "2.7153160572052\n",
      "2.7514424324035645\n",
      "2.7707509994506836\n",
      "2.6662869453430176\n",
      "2.796515941619873\n",
      "2.598062753677368\n",
      "2.783961772918701\n",
      "2.6404531002044678\n",
      "2.597564935684204\n",
      "2.601882219314575\n",
      "2.6659204959869385\n",
      "2.9092514514923096\n",
      "2.653740406036377\n",
      "2.756934642791748\n",
      "2.7490105628967285\n",
      "2.7826764583587646\n",
      "2.722247838973999\n",
      "2.715773820877075\n",
      "2.7618682384490967\n",
      "2.7958450317382812\n",
      "2.8703460693359375\n",
      "2.6742074489593506\n",
      "2.6863808631896973\n",
      "2.582412004470825\n",
      "2.708669900894165\n",
      "2.8786823749542236\n",
      "2.84647274017334\n",
      "2.7045466899871826\n",
      "2.8746368885040283\n",
      "2.744877338409424\n",
      "2.67295241355896\n",
      "2.6560444831848145\n",
      "2.71267032623291\n",
      "2.7171268463134766\n",
      "2.6853206157684326\n",
      "2.7247447967529297\n",
      "2.560987949371338\n",
      "2.711902379989624\n",
      "2.829362392425537\n",
      "2.8059701919555664\n",
      "2.75359845161438\n",
      "2.759718418121338\n",
      "2.785773515701294\n",
      "2.5458476543426514\n",
      "2.7245519161224365\n",
      "2.6731417179107666\n",
      "2.57783579826355\n",
      "2.6684799194335938\n",
      "2.879692316055298\n",
      "2.6114048957824707\n",
      "2.609992027282715\n",
      "2.5642600059509277\n",
      "2.6746535301208496\n",
      "2.7533981800079346\n",
      "2.639573812484741\n",
      "2.6007726192474365\n",
      "2.6472034454345703\n",
      "2.682486057281494\n",
      "2.7399377822875977\n",
      "2.636411666870117\n",
      "2.7424607276916504\n",
      "2.8836731910705566\n",
      "2.6909120082855225\n",
      "2.665843963623047\n",
      "2.5230116844177246\n",
      "2.7008917331695557\n",
      "2.709834575653076\n",
      "2.7016944885253906\n",
      "2.8103933334350586\n",
      "2.76627779006958\n",
      "2.5393497943878174\n",
      "2.6314423084259033\n",
      "2.7402503490448\n",
      "2.738919973373413\n",
      "2.86430287361145\n",
      "2.7078614234924316\n",
      "2.775770425796509\n",
      "2.6907315254211426\n",
      "2.9234161376953125\n",
      "2.6341652870178223\n",
      "2.686286687850952\n",
      "2.6506924629211426\n",
      "2.6442012786865234\n",
      "2.6677918434143066\n",
      "2.721715211868286\n",
      "2.504251480102539\n",
      "2.6842894554138184\n",
      "2.7047324180603027\n",
      "2.6924078464508057\n",
      "2.697005033493042\n",
      "2.781001567840576\n",
      "2.7263846397399902\n",
      "2.655726432800293\n",
      "2.6401028633117676\n",
      "2.6227095127105713\n",
      "2.6706511974334717\n",
      "2.7251174449920654\n",
      "2.647383213043213\n",
      "2.594074249267578\n",
      "2.629958391189575\n",
      "2.6808125972747803\n",
      "2.7890608310699463\n",
      "2.680365800857544\n",
      "2.6108596324920654\n",
      "2.624882459640503\n",
      "2.790116310119629\n",
      "2.5648794174194336\n",
      "2.81488299369812\n",
      "2.8579955101013184\n",
      "2.7869791984558105\n",
      "2.861815929412842\n",
      "2.599977731704712\n",
      "2.651909828186035\n",
      "2.653231143951416\n",
      "2.737485408782959\n",
      "2.630134344100952\n",
      "2.578273057937622\n",
      "2.857281446456909\n",
      "2.714571952819824\n",
      "2.596883773803711\n",
      "2.586606025695801\n",
      "2.692873477935791\n",
      "2.558289051055908\n",
      "2.63015079498291\n",
      "2.6717677116394043\n",
      "2.8760716915130615\n",
      "2.674753189086914\n",
      "2.7548110485076904\n",
      "2.7227776050567627\n",
      "2.65915584564209\n",
      "2.830606460571289\n",
      "2.5529043674468994\n",
      "2.6457695960998535\n",
      "2.6601738929748535\n",
      "2.83442759513855\n",
      "2.829862594604492\n",
      "2.552220582962036\n",
      "2.632509708404541\n",
      "2.648151397705078\n",
      "2.85817289352417\n",
      "2.7589550018310547\n",
      "2.7257442474365234\n",
      "2.5505402088165283\n",
      "2.576350688934326\n",
      "2.758864402770996\n",
      "2.6469602584838867\n",
      "2.636103868484497\n",
      "2.6734793186187744\n",
      "2.5502123832702637\n",
      "2.637449264526367\n",
      "2.6405842304229736\n",
      "2.5869710445404053\n",
      "2.7628116607666016\n",
      "2.6193201541900635\n",
      "2.6070144176483154\n",
      "2.6992642879486084\n",
      "2.6271092891693115\n",
      "2.660336971282959\n",
      "2.706010580062866\n",
      "2.735673666000366\n",
      "2.6158621311187744\n",
      "2.7516379356384277\n",
      "2.602588176727295\n",
      "2.6313207149505615\n",
      "2.630467653274536\n",
      "2.779094934463501\n",
      "2.6704413890838623\n",
      "2.671307325363159\n",
      "2.7444722652435303\n",
      "2.6556272506713867\n",
      "2.621133804321289\n",
      "2.692180633544922\n",
      "2.6697540283203125\n",
      "2.5548975467681885\n",
      "2.713279962539673\n",
      "2.7671144008636475\n",
      "2.7430460453033447\n",
      "2.6461374759674072\n",
      "2.653801202774048\n",
      "2.810988664627075\n",
      "2.694643974304199\n",
      "2.847973346710205\n",
      "2.627692699432373\n",
      "2.6449873447418213\n",
      "2.9275965690612793\n",
      "2.670628786087036\n",
      "2.697518825531006\n",
      "2.5419647693634033\n",
      "2.6340792179107666\n",
      "2.6064698696136475\n",
      "2.6582393646240234\n",
      "2.772456169128418\n",
      "2.594575881958008\n",
      "2.596067428588867\n",
      "2.4599757194519043\n",
      "2.680913209915161\n",
      "2.6449472904205322\n",
      "2.5184671878814697\n",
      "2.624878168106079\n",
      "2.6994802951812744\n",
      "2.704230785369873\n",
      "2.609360694885254\n",
      "2.70218825340271\n",
      "2.615579605102539\n",
      "2.675837755203247\n",
      "2.5911684036254883\n",
      "2.664299249649048\n",
      "2.71648907661438\n",
      "2.651569366455078\n",
      "2.6246297359466553\n",
      "2.7008514404296875\n",
      "2.5890791416168213\n",
      "2.650531530380249\n",
      "2.757530450820923\n",
      "2.5113539695739746\n",
      "2.589052200317383\n",
      "2.6568756103515625\n",
      "2.6566953659057617\n",
      "2.5990328788757324\n",
      "2.7071421146392822\n",
      "2.6641764640808105\n",
      "2.731523275375366\n",
      "2.5892326831817627\n",
      "2.7628440856933594\n",
      "2.728276491165161\n",
      "2.8035848140716553\n",
      "2.5880987644195557\n",
      "2.59187650680542\n",
      "2.730006694793701\n",
      "2.7347488403320312\n",
      "2.7763142585754395\n",
      "2.6201488971710205\n",
      "2.5922961235046387\n",
      "2.53450870513916\n",
      "2.712664842605591\n",
      "2.605130910873413\n",
      "2.520106554031372\n",
      "2.652832508087158\n",
      "2.5836124420166016\n",
      "2.743844509124756\n",
      "2.7941837310791016\n",
      "2.730402708053589\n",
      "2.748678207397461\n",
      "2.702901840209961\n",
      "2.6580193042755127\n",
      "2.663834810256958\n",
      "2.6821324825286865\n",
      "2.6938233375549316\n",
      "2.5686538219451904\n",
      "2.6842525005340576\n",
      "2.6096160411834717\n",
      "2.691016674041748\n",
      "2.5890986919403076\n",
      "2.6558165550231934\n",
      "2.6982152462005615\n",
      "2.529083251953125\n",
      "2.771584987640381\n",
      "2.799288272857666\n",
      "2.805732011795044\n",
      "2.4608776569366455\n",
      "2.724520683288574\n",
      "2.5619215965270996\n",
      "2.452244281768799\n",
      "2.715895414352417\n",
      "2.63148832321167\n",
      "2.577569007873535\n",
      "2.7849693298339844\n",
      "2.610234022140503\n",
      "2.703878164291382\n",
      "2.8536746501922607\n",
      "2.545944929122925\n",
      "2.506148338317871\n",
      "2.7913405895233154\n",
      "2.578282117843628\n",
      "2.625074625015259\n",
      "2.6376256942749023\n",
      "2.5596296787261963\n",
      "2.6361217498779297\n",
      "2.5769364833831787\n",
      "2.6219542026519775\n",
      "2.4794070720672607\n",
      "2.5251047611236572\n",
      "2.626221179962158\n",
      "2.6445438861846924\n",
      "2.5909745693206787\n",
      "2.6943747997283936\n",
      "2.495952844619751\n",
      "2.786202907562256\n",
      "2.5996809005737305\n",
      "2.57397723197937\n",
      "2.5252022743225098\n",
      "2.6868653297424316\n",
      "2.718909740447998\n",
      "2.7319536209106445\n",
      "2.4932587146759033\n",
      "2.739161252975464\n",
      "2.5544745922088623\n",
      "2.6839566230773926\n",
      "2.5690770149230957\n",
      "2.766930341720581\n",
      "2.5420849323272705\n",
      "2.751586437225342\n",
      "2.5953948497772217\n",
      "2.608771324157715\n",
      "2.4083142280578613\n",
      "2.636983871459961\n",
      "2.6237690448760986\n",
      "2.6990396976470947\n",
      "2.662004232406616\n",
      "2.688668727874756\n",
      "2.738344669342041\n",
      "2.636317014694214\n",
      "2.647446393966675\n",
      "2.6364383697509766\n",
      "2.544318914413452\n",
      "2.6291024684906006\n",
      "2.681591749191284\n",
      "2.649597644805908\n",
      "2.7045516967773438\n",
      "2.4986000061035156\n",
      "2.627918243408203\n",
      "2.6459193229675293\n",
      "2.572817325592041\n",
      "2.659108877182007\n",
      "2.5663154125213623\n",
      "2.682379722595215\n",
      "2.6585090160369873\n",
      "2.6518592834472656\n",
      "2.5506811141967773\n",
      "2.672879457473755\n",
      "2.669614791870117\n",
      "2.4459474086761475\n",
      "2.622307777404785\n",
      "2.6020920276641846\n",
      "2.4568185806274414\n",
      "2.5900416374206543\n",
      "2.617013931274414\n",
      "2.621248483657837\n",
      "2.6486804485321045\n",
      "2.4411590099334717\n",
      "2.5785953998565674\n",
      "2.5730080604553223\n",
      "2.5332207679748535\n",
      "2.641333818435669\n",
      "2.6315813064575195\n",
      "2.7375106811523438\n",
      "2.5450267791748047\n",
      "2.6659319400787354\n",
      "2.6840264797210693\n",
      "2.5853466987609863\n",
      "2.506650686264038\n",
      "2.684091329574585\n",
      "2.6629998683929443\n",
      "2.6461727619171143\n",
      "2.6142032146453857\n",
      "2.6604487895965576\n",
      "2.7112834453582764\n",
      "2.6787240505218506\n",
      "2.599514961242676\n",
      "2.6411280632019043\n",
      "2.5214266777038574\n",
      "2.6317195892333984\n",
      "2.5467307567596436\n",
      "2.6995115280151367\n",
      "2.5500707626342773\n",
      "2.5144076347351074\n",
      "2.644930601119995\n",
      "2.6071951389312744\n",
      "2.6608829498291016\n",
      "2.729949474334717\n",
      "2.560908317565918\n",
      "2.6293723583221436\n",
      "2.6620707511901855\n",
      "2.619781494140625\n",
      "2.5629749298095703\n",
      "2.597442388534546\n",
      "2.7479360103607178\n",
      "2.6105120182037354\n",
      "2.601202964782715\n",
      "2.7193596363067627\n",
      "2.652106523513794\n",
      "2.6388814449310303\n",
      "2.6139867305755615\n",
      "2.5140140056610107\n",
      "2.5554308891296387\n",
      "2.5717785358428955\n",
      "2.680448293685913\n",
      "2.5537517070770264\n",
      "2.4974324703216553\n",
      "2.4868006706237793\n",
      "2.569967031478882\n",
      "2.5779879093170166\n",
      "2.6902148723602295\n",
      "2.6553568840026855\n",
      "2.5777409076690674\n",
      "2.548525333404541\n",
      "2.71516489982605\n",
      "2.5246293544769287\n",
      "2.511009693145752\n",
      "2.593595027923584\n",
      "2.57615065574646\n",
      "2.491215229034424\n",
      "2.5709304809570312\n",
      "2.4839255809783936\n",
      "2.5738964080810547\n",
      "2.5341756343841553\n",
      "2.7355430126190186\n",
      "2.6040682792663574\n",
      "2.5173227787017822\n",
      "2.5133185386657715\n",
      "2.5126168727874756\n",
      "2.580871343612671\n",
      "2.5247750282287598\n",
      "2.7191710472106934\n",
      "2.577291250228882\n",
      "2.684969425201416\n",
      "2.6745643615722656\n",
      "2.5642635822296143\n",
      "2.606886625289917\n",
      "2.656606912612915\n",
      "2.6986494064331055\n",
      "2.5503647327423096\n",
      "2.704075813293457\n",
      "2.6580162048339844\n",
      "2.5547220706939697\n",
      "2.4833004474639893\n",
      "2.6159377098083496\n",
      "2.4245991706848145\n",
      "2.5766236782073975\n",
      "2.5586211681365967\n",
      "2.4937093257904053\n",
      "2.454892158508301\n",
      "2.7199454307556152\n",
      "2.575032949447632\n",
      "2.633833169937134\n",
      "2.416703939437866\n",
      "2.7121002674102783\n",
      "2.6091458797454834\n",
      "2.4616639614105225\n",
      "2.540283679962158\n",
      "2.558800458908081\n",
      "2.586390733718872\n",
      "2.589380979537964\n",
      "2.572188138961792\n",
      "2.707275390625\n",
      "2.71871018409729\n",
      "2.724647283554077\n",
      "2.84599232673645\n",
      "2.6030802726745605\n",
      "2.60109281539917\n",
      "2.5271008014678955\n",
      "2.694331169128418\n",
      "2.769223690032959\n",
      "2.623600959777832\n",
      "2.5002102851867676\n",
      "2.743939161300659\n",
      "2.6951489448547363\n",
      "2.3758721351623535\n",
      "2.53717303276062\n",
      "2.6927239894866943\n",
      "2.6522293090820312\n",
      "2.5561113357543945\n",
      "2.585844039916992\n",
      "2.5745506286621094\n",
      "2.555650472640991\n",
      "2.6434450149536133\n",
      "2.5615663528442383\n",
      "2.6829276084899902\n",
      "2.549642562866211\n",
      "2.6234920024871826\n",
      "2.6072752475738525\n",
      "2.5399210453033447\n",
      "2.498110771179199\n",
      "2.597696542739868\n",
      "2.5274975299835205\n",
      "2.483016014099121\n",
      "2.6172444820404053\n",
      "2.5120558738708496\n",
      "2.6997122764587402\n",
      "2.5923941135406494\n",
      "2.617696523666382\n",
      "2.6063623428344727\n",
      "2.4907870292663574\n",
      "2.659085512161255\n",
      "2.4788687229156494\n",
      "2.644885540008545\n",
      "2.484689712524414\n",
      "2.697378158569336\n",
      "2.5595667362213135\n",
      "2.6674118041992188\n",
      "2.5334603786468506\n",
      "2.7074198722839355\n",
      "2.7862656116485596\n",
      "2.5464284420013428\n",
      "2.554943799972534\n",
      "2.5693752765655518\n",
      "2.530170440673828\n",
      "2.5669076442718506\n",
      "2.6056759357452393\n",
      "2.488901138305664\n"
     ]
    }
   ],
   "source": [
    "# Train the Bigram Model\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n",
    "batch_size = 32\n",
    "for steps in range(1000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we can see that our generated output possess much more characteristics of a real text written by a human, however since the model is only a bigram model (predicting the next word based on the previous word), the model will not be as accurate as we would have hoped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WAIkpre,\n",
      "APoE't te meNGa?oYoung ENoy B.\n",
      "\n",
      "meat ik, w wiyare, arfy IN.\n",
      "USad taen? qouthor c\n",
      "ORe I3:\n",
      "Thir.\n",
      "y\n",
      "T s s s mare:\n",
      " p rAnll'veante's\n",
      "Bwow ous secol be nd utek, a mdsengag owenshonn he mechinlns ayestawit d  b t s llllern orud am.\n",
      "I I ongf t.\n",
      "M whog adive tiuFphe f se t aend wurerord thhamtunof athie cor CENGon hicr herererlrhar'xhe\n",
      "Th al yomay lseet yole\n",
      "I'n pe t othithen bE wELEy a ke!ay\n",
      "\n",
      "MIVe sth hatonon\n",
      "Tond, g s thend, buneomyEQesoritoumsitDKEEuris antrese DENdivantewo;oeWe chu bhidowin\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Transformer Model\n",
    "- Using what we have learnt previously, we implement self-attention here.\n",
    "- From the matrix below, we can see that the sum of each row will add up to 1.\n",
    "- The softmax of negative infinity is equal to 0.\n",
    "- Higher dot product will mean higher affinity -> neural network have higher attenuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimension of the value tensor is: torch.Size([4, 8, 16])\n",
      "The Dimension of the output tensor is: torch.Size([4, 8, 16])\n",
      "The Dimension of the weight tensor is: torch.Size([4, 8, 8])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# Transpose the last two dimensions. \n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # The mask feature will always be present in the decoder block to allow the words to talk to each other.\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "print(\"The Dimension of the value tensor is: \"+str(v.shape))\n",
    "#print(v[0])\n",
    "print(\"The Dimension of the output tensor is: \"+str(out.shape))\n",
    "#print(out[0])\n",
    "print(\"The Dimension of the weight tensor is: \"+str(wei.shape))\n",
    "print(wei[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention is a communication mechnism. Can be seen as nodes in a directed graph relating to each other in terms of \"closeness\" and aggregating information with a weighted sum from all nodes that point to them. With these weights the attention mechnism is able to easily represent any form of directed graphs however are not capable of representing positional data unless otherwisely instructed.\n",
    "- People also tried establishing links between [GNNs and Transformers](https://graphdeeplearning.github.io/post/transformers-are-gnns/) (However the Transformers use self-attention which is different to the standard-attention mechanism for GAT, however the two architecture are built on similar level of understanding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "# We can now scale the variance using the method proposed in the original Attention is all you need thesis to reforge the variance to approximately 1.\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Masked multi-head attention\n",
    "Why do we need Masked multi-head attention for the decoders in our transformer framework?\n",
    "- Because Transformer just like the RNN is a autoregressive function. This means when generating a given response, it will predicts the future word with respect to what it predicted previously, if we set the time dimension to 8. This means it should only be able to maintain an attention-span of 8 units and will not be able to see the future words.\n",
    "- A better explanation from [stackexchange](https://stats.stackexchange.com/questions/634864/why-do-we-mask-input-tokens-for-the-decoder-in-a-transformer):\n",
    "  - _The sequence we feed to the decoder always has to be the same length. So when we feed the decoder a sequence the sequence must be \"full-length\". In this kind of transformer the generation of tokens, happens by the decoder always predicting the next token, based on the already generated tokens. However, because we have to use as input a complete sequence and not only what has been generated so far, the Decoder could \"look\" at the words that are right of the already generated tokens. In order to avoid this everything right f the token that is to be predicted will be masked._\n",
    "\n",
    "<p align=\"center\" style=\"background-color: white;\">\n",
    "        <img src=\"images/fig13.gif\" alt=\"fishy\" class=\"bg-primary\" width=\"500px\" />\n",
    "        <figcaption align=\"centre\" >\n",
    "            <a href=\"https://jalammar.github.io/illustrated-transformer/\">Illustrated Transformer</a>\n",
    "          </figcaption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Layer Normalization\n",
    "\n",
    "- Layer normalization ensures all the activations in each layer to have a mean of zero and a unit (1) variance. It is proposed in the paper \"[Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\".\n",
    "\n",
    "$$\n",
    "h_i = \\frac{g}{\\sigma}(h_i - \\mu)\n",
    "$$\n",
    "\n",
    "- g: the gain variable (can be set to 1).\n",
    "- $\\mu$: the mean\n",
    "- $\\sigma$: the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now assume we have the following fc layer described by the equation:\n",
    "\n",
    "$$\n",
    "x' = f[{w_1}^{T}x+b_1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "h = \\gamma_{1}[\\frac{x'-\\mu_{1}}{\\sigma_{1}}]+\\beta_{1}\n",
    "$$\n",
    "\n",
    "- where $\\gamma_{1}$ and $\\beta_{1}$ are trainable parameters, which can be trained and changed in order to optimize our loss function.\n",
    "- If we are to get an output of 2 words with 3 dimensions, we are going to average the mean with respect to each words across the three dimensions to compute the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n",
    "        self.eps  = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim=True)\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x - xmean)/torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_aside:_ When the Keep Dimension is equal to true. this will ensure the structure of the matrix is not destroyed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6591],\n",
      "        [-0.8219],\n",
      "        [-0.1061],\n",
      "        [-0.5706]])\n",
      "tensor([-0.6591, -0.8219, -0.1061, -0.5706])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "amean_kt = a.mean(1, keepdim=True)\n",
    "amean_kf = a.mean(1, keepdim=False)\n",
    "\n",
    "print(amean_kt)\n",
    "print(amean_kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variables\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2984, val loss 4.2951\n",
      "step 100: train loss 2.6478, val loss 2.6505\n",
      "step 200: train loss 2.5055, val loss 2.5142\n",
      "step 300: train loss 2.4207, val loss 2.4302\n",
      "step 400: train loss 2.3732, val loss 2.3725\n",
      "step 500: train loss 2.3250, val loss 2.3322\n",
      "step 600: train loss 2.2722, val loss 2.2747\n",
      "step 700: train loss 2.2222, val loss 2.2441\n",
      "step 800: train loss 2.1830, val loss 2.2153\n",
      "step 900: train loss 2.1352, val loss 2.1736\n",
      "step 1000: train loss 2.1051, val loss 2.1410\n",
      "step 1100: train loss 2.0858, val loss 2.1184\n",
      "step 1200: train loss 2.0413, val loss 2.0896\n",
      "step 1300: train loss 2.0162, val loss 2.0792\n",
      "step 1400: train loss 2.0007, val loss 2.0570\n",
      "step 1500: train loss 1.9818, val loss 2.0421\n",
      "step 1600: train loss 1.9540, val loss 2.0391\n",
      "step 1700: train loss 1.9284, val loss 2.0243\n",
      "step 1800: train loss 1.9173, val loss 2.0066\n",
      "step 1900: train loss 1.8877, val loss 1.9950\n",
      "step 2000: train loss 1.8812, val loss 1.9750\n",
      "step 2100: train loss 1.8671, val loss 1.9681\n",
      "step 2200: train loss 1.8696, val loss 1.9709\n",
      "step 2300: train loss 1.8316, val loss 1.9404\n",
      "step 2400: train loss 1.8275, val loss 1.9443\n",
      "step 2500: train loss 1.8243, val loss 1.9289\n",
      "step 2600: train loss 1.8068, val loss 1.9170\n",
      "step 2700: train loss 1.8064, val loss 1.9288\n",
      "step 2800: train loss 1.7683, val loss 1.9169\n",
      "step 2900: train loss 1.7669, val loss 1.9000\n",
      "step 3000: train loss 1.7759, val loss 1.9135\n",
      "step 3100: train loss 1.7617, val loss 1.9027\n",
      "step 3200: train loss 1.7456, val loss 1.8933\n",
      "step 3300: train loss 1.7376, val loss 1.8725\n",
      "step 3400: train loss 1.7458, val loss 1.8855\n",
      "step 3500: train loss 1.7157, val loss 1.8567\n",
      "step 3600: train loss 1.7082, val loss 1.8658\n",
      "step 3700: train loss 1.7094, val loss 1.8555\n",
      "step 3800: train loss 1.7125, val loss 1.8598\n",
      "step 3900: train loss 1.7034, val loss 1.8441\n",
      "step 4000: train loss 1.6868, val loss 1.8472\n",
      "step 4100: train loss 1.7032, val loss 1.8485\n",
      "step 4200: train loss 1.6995, val loss 1.8428\n",
      "step 4300: train loss 1.6767, val loss 1.8427\n",
      "step 4400: train loss 1.6876, val loss 1.8217\n",
      "step 4500: train loss 1.6860, val loss 1.8197\n",
      "step 4600: train loss 1.6732, val loss 1.8221\n",
      "step 4700: train loss 1.6671, val loss 1.8343\n",
      "step 4800: train loss 1.6644, val loss 1.8251\n",
      "step 4900: train loss 1.6629, val loss 1.8173\n",
      "step 4999: train loss 1.6612, val loss 1.8087\n",
      "\n",
      "And they trudch?\n",
      "\n",
      "STANLOLY:\n",
      "Were is thy beough and Onath me?\n",
      "I and barnith foul him vellaced?\n",
      "\n",
      "DUKEen will my feanst, when mus\n",
      "Youngellof it her than well in a\n",
      "grivee-sen contlatiH\n",
      "Give vittornound long: will just, leling me upolns;\n",
      "Honching prup the spent yem:' make normord?\n",
      "\n",
      "MERCUMP\n",
      "\n",
      "GLOUCK:\n",
      "MeangerouW-not when evily our them,\n",
      "His hands in his soul\n",
      "And strume known, rup af so;\n",
      "Angle when think all of whiped are guil? Hereful be!\n",
      "\n",
      "GREY:\n",
      "Had advalm, I Edwas diding must are no Runry\n",
      "more my carend neely Pome, go.\n",
      "You come and me\n",
      "That he severen my hearther.\n",
      "\n",
      "CORIOLANUS:\n",
      "Where not us body.\n",
      "\n",
      "Think Cospects:\n",
      "Decas lord\n",
      "I canse than wors.\n",
      "\n",
      "Lueran'd Plavove Lady passed of's courted; How\n",
      "He in deast you Rome, soy, come,\n",
      "Aguiss news with hear neen our more him.\n",
      "I would fravence way so! why love ne'er the well not a verwicks\n",
      "Lord, he demilurbund hinger, do thity\n",
      "Though to this dether of the summe wippy and Poubjot, and, could pectless kings that knew you, delict do it\n",
      "onout in for Gly thee in eneman;\n",
      "But boy hears, I hus not whench out\n",
      "And hid constattanous, with yet or love\n",
      "As be rest mines softends his in it. Hould this fastor now,\n",
      "An bear wand bencaunI hast a caning and and the any inkeling lament!\n",
      "\n",
      "FLORIZERCY:\n",
      "Oxfordness what see wo I did; the stroid's lesters to Tumlals,\n",
      "How the of the worjur'd Worth, through he call's make him this counce.\n",
      "\n",
      "Proved:\n",
      "And the nor counce be-dague of vonews, Rome,\n",
      "Most viccance him more him.\n",
      "\n",
      "RICHERR YORK:\n",
      "Be you therege, is of eyes,\n",
      "But wall latten else me it justious blether, say, as;\n",
      "Rome, thou averts and bunt.\n",
      "\n",
      "Pretengle:\n",
      "My farew-rieng rebremy aclers in on my Lord.\n",
      "\n",
      "&ISHAM:\n",
      "Give word make a roud upon?\n",
      "\n",
      "JULIET:\n",
      "In your headly, ah, sinccass o' not hither.\n",
      "\n",
      "Fries her thengs this flanted friend,\n",
      "A cormous chip, o'emon his our? All, what here with fair?\n",
      "But of the burse like in thelw hear punict\n",
      "Ap house the a down thin with us wouly proud,\n",
      "They and sentrengand the never do,\n",
      "Juse Reparding: I will down'd fight his suppect, when time ou\n",
      "all \n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bise lanture to the birds to cheep\n",
      "For legream'd you not me, so the dead must have\n",
      "And the countle upon that pard I't have leck of I wencome amble\n",
      "Bus regon her sidons it his duke!\n",
      "Boting fastem ofther his could pleason of my quickness; my lord,\n",
      "As hence uponce! and hone man unter some softed,\n",
      "Hence doney meance a rong approus;\n",
      "Sich.\n",
      "\n",
      "DUKE OF OF York, knownst an my plent you do hellow\n",
      "'Cour deap tour libuther'd, none deam longs and this wounderiting,\n",
      "And roher denille formound most; quuke man of you love: let.\n",
      "You hast founds to go me, this trumber\n",
      "make and pless and exver of child.\n",
      "\n",
      "Find:\n",
      "I way for the my atcrad: for thy landry,\n",
      "That what dewouch his shall not foreht dother were ingeen,\n",
      "This shance set proud, it must, and you round,\n",
      "And with arm fortump----it it should plrip out father waptantisfence\n",
      "To cruny father relliked upon time win in Done\n",
      "Weemp of bence you.- Souy conturn'd humbling, your hide\n",
      "As hillough procling not of the were red;\n",
      "Were to ther forth'd, thing wears\n",
      "Which plerford;\n",
      "Come my leff dutiounsling, shall,\n",
      "I knound what so, ince in this all, and whose\n",
      "Has shall thy are willandure it spenggreess.\n",
      "\n",
      "BRUTUS:\n",
      "I play him rup tio, genel; and Bygen,\n",
      "Now all rigl'd these love of dest.\n",
      "Here you wild Tun your good give and Velice,\n",
      "Frup ancladom, sars and befellate it!\n",
      "Then the what jost he merve.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Of love their sorrow?\n",
      "Thereforew not I goor, with a Pagch?\n",
      "\n",
      "KING RICHARD III:\n",
      "Mane your much unathousand, good the care gavexple one Henry'll won.\n",
      "\n",
      "HESSening buld never nermal in name\n",
      "Of orse youngs. Chatchmad visentian cusland you,\n",
      "And I say man the with most of my cople,\n",
      "But never grous placiouseng hands, yet my and and Handurp;\n",
      "baster hour held in she wake you not\n",
      "Musire to but a pustile unbef' deast honour,\n",
      "Well coursain extenceed strengs, God,\n",
      "Win our when wquent-hich and you nob!\n",
      "Her they day wounded of you cont;\n",
      "You known dight a the dayful thou hede thou counse the wands\n",
      "Yea setter heir in thusawful him wrick;\n",
      "Come and me and must in the will w\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
